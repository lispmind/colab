{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRcwj/yu0lyB7qg/swz9EV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lispmind/colab/blob/main/hf_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V80piya5CLWE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n"
      ],
      "metadata": {
        "id": "4hbSsNfWCWXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "from datasets import load_dataset, Audio\n",
        "\n",
        "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))\n",
        "result = speech_recognizer(dataset[:4][\"audio\"])\n",
        "print([d[\"text\"] for d in result])\n"
      ],
      "metadata": {
        "id": "cOWVcxy9CdVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "encoding = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "rmkCJk4CD_9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "transcriber = pipeline(task=\"automatic-speech-recognition\")\n",
        "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n"
      ],
      "metadata": {
        "id": "lstb4D5aEpSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcriber = pipeline(model=\"openai/whisper-large-v2\")\n",
        "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
      ],
      "metadata": {
        "id": "uOzXsKUVEw6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "vision_classifier = pipeline(model=\"google/vit-base-patch16-224\")\n",
        "preds = vision_classifier(\n",
        "    images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
        ")\n",
        "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
        "preds"
      ],
      "metadata": {
        "id": "tWfLjEewFm7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install einops\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.set_default_device(\"cuda\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
        "\n",
        "inputs = tokenizer('''def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)\n",
        "\n"
      ],
      "metadata": {
        "id": "wFVJQPH4F1DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"\n",
        "   for i in range(2, n+1):\n",
        "       for j in range(2, i):\n",
        "           if i % j == 0:\n",
        "               break\n",
        "       else:\n",
        "           print(i)\n",
        "\n",
        "print_prime(20)"
      ],
      "metadata": {
        "id": "bzgLqelwHXZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_even(numbers):\n",
        "    \"\"\"\n",
        "    Returns the sum of all even numbers in the list\n",
        "    \"\"\"\n",
        "    return sum(filter(lambda x: x % 2 == 0, numbers))\n",
        "\n",
        "print(sum_even([1, 2, 3, 4, 5, 6])) # Output: 12"
      ],
      "metadata": {
        "id": "Y5G5ln1EHYAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -q transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"bigcode/starcoder\"\n",
        "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
        "\n",
        "inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "rK4gBUuHLraW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}